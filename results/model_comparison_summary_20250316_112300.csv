model_name,model_type,parameters,dataset,accuracy,reasoning_length,inference_time,instruction_type,completion_rate,error_rate
phi4:latest,Phi,4.2B,TruthfulQA MC1,66.09547123623011,16.041615667074662,2.8233429235810896,cod_instruction,100.0,0.0
phi4:latest,Phi,4.2B,TruthfulQA MC1,65.97307221542228,19.46266829865361,2.780421020059574,standard_instruction,100.0,0.0
phi4:latest,Phi,4.2B,TruthfulQA MC1,68.91064871481028,182.22888616891066,15.20096208415971,cot_instruction,100.0,0.0
llama3.2:latest,Llama,8B,TruthfulQA MC1,44.92044063647491,4.660954712362301,0.4915828932455149,cod_instruction,100.0,0.0
llama3.2:latest,Llama,8B,TruthfulQA MC1,48.71481028151774,9.915544675642595,0.5962628690819991,standard_instruction,100.0,0.0
llama3.2:latest,Llama,8B,TruthfulQA MC1,52.01958384332925,61.52264381884945,1.4805849698649187,cot_instruction,100.0,0.0
qwq:latest,QWQ,Unknown,TruthfulQA MC1,70.62423500611995,37.16871921182266,8.704836821614816,cod_instruction,99.38800489596083,0.6119951040391677
qwq:latest,QWQ,Unknown,TruthfulQA MC1,72.2154222766218,8.695331695331696,3.5291960031453162,standard_instruction,99.6328029375765,0.36719706242350064
qwq:latest,QWQ,Unknown,TruthfulQA MC1,75.27539779681763,96.47901234567901,15.810700596114735,cot_instruction,99.14320685434517,0.8567931456548347
deepseek-r1:14b,DeepSeek,14B,TruthfulQA MC1,54.589963280293766,3.134638922888617,1.664245140304472,cod_instruction,100.0,0.0
deepseek-r1:14b,DeepSeek,14B,TruthfulQA MC1,57.52753977968176,0.17503059975520197,1.4756954661781376,standard_instruction,100.0,0.0
deepseek-r1:14b,DeepSeek,14B,TruthfulQA MC1,57.16034271725826,12.314565483476132,2.3317061926772866,cot_instruction,100.0,0.0
qwen2.5:14b,Qwen,14B,TruthfulQA MC1,63.28029375764994,4.1150550795593634,1.6109029099874135,cod_instruction,100.0,0.0
qwen2.5:14b,Qwen,14B,TruthfulQA MC1,70.62423500611995,0.8347613219094248,1.0323884300386015,standard_instruction,100.0,0.0
qwen2.5:14b,Qwen,14B,TruthfulQA MC1,71.60342717258263,69.70991432068543,5.233121502793404,cot_instruction,100.0,0.0
llama3.1:8b,Llama,8B,TruthfulQA MC1,44.67564259485924,14.71970624235006,1.2419789470686615,cod_instruction,100.0,0.0
llama3.1:8b,Llama,8B,TruthfulQA MC1,50.55079559363526,0.26438188494492043,0.6669643511824683,standard_instruction,100.0,0.0
llama3.1:8b,Llama,8B,TruthfulQA MC1,51.65238678090576,58.70257037943696,2.4950597831931516,cot_instruction,100.0,0.0
