model_name,model_type,parameters,dataset,accuracy,reasoning_length,inference_time,instruction_type,completion_rate,error_rate
phi4:latest,Phi,4.2B,TruthfulQA MC1,66.09547123623011,16.041615667074662,2.7827924514779845,cod_instruction,100.0,0.0
phi4:latest,Phi,4.2B,TruthfulQA MC1,65.97307221542228,19.46266829865361,2.816186691877162,standard_instruction,100.0,0.0
phi4:latest,Phi,4.2B,TruthfulQA MC1,68.91064871481028,182.22888616891066,18.018400110842403,cot_instruction,100.0,0.0
llama3.2:latest,Llama,8B,TruthfulQA MC1,44.92044063647491,4.660954712362301,0.5271457213374947,cod_instruction,100.0,0.0
llama3.2:latest,Llama,8B,TruthfulQA MC1,48.71481028151774,9.915544675642595,0.6514200029058002,standard_instruction,100.0,0.0
llama3.2:latest,Llama,8B,TruthfulQA MC1,52.01958384332925,61.52264381884945,1.6594373149708406,cot_instruction,100.0,0.0
qwq:latest,QWQ,Unknown,TruthfulQA MC1,70.50183598531213,35.81111111111111,9.742147505430527,cod_instruction,99.14320685434517,0.8567931456548347
qwq:latest,QWQ,Unknown,TruthfulQA MC1,72.2154222766218,8.71990171990172,3.919235343429322,standard_instruction,99.6328029375765,0.36719706242350064
qwq:latest,QWQ,Unknown,TruthfulQA MC1,75.27539779681763,94.90954151177199,18.209148214947632,cot_instruction,98.77600979192167,1.2239902080783354
deepseek-r1:14b,DeepSeek,14B,TruthfulQA MC1,54.589963280293766,3.134638922888617,1.844327685590765,cod_instruction,100.0,0.0
deepseek-r1:14b,DeepSeek,14B,TruthfulQA MC1,57.52753977968176,0.17503059975520197,1.619324944740118,standard_instruction,100.0,0.0
deepseek-r1:14b,DeepSeek,14B,TruthfulQA MC1,57.16034271725826,12.314565483476132,2.572311582296831,cot_instruction,100.0,0.0
qwen2.5:14b,Qwen,14B,TruthfulQA MC1,63.28029375764994,4.1150550795593634,1.766166796153149,cod_instruction,100.0,0.0
qwen2.5:14b,Qwen,14B,TruthfulQA MC1,70.62423500611995,0.8347613219094248,1.1292289564046312,standard_instruction,100.0,0.0
qwen2.5:14b,Qwen,14B,TruthfulQA MC1,71.60342717258263,69.70991432068543,5.623716368085751,cot_instruction,100.0,0.0
llama3.1:8b,Llama,8B,TruthfulQA MC1,44.67564259485924,14.71970624235006,1.3139204439072159,cod_instruction,100.0,0.0
llama3.1:8b,Llama,8B,TruthfulQA MC1,50.55079559363526,0.26438188494492043,0.723757223808634,standard_instruction,100.0,0.0
llama3.1:8b,Llama,8B,TruthfulQA MC1,51.65238678090576,58.70257037943696,2.7726056100105274,cot_instruction,100.0,0.0
